Домашнее задание 1: Проектирование базовой архитектуры данных
Задача:
Спроектировать и описать базовую архитектуру данных для вымышленного или
реального сервиса/приложения по вашему выбору. Архитектура должна охватывать
полный цикл: сбор, хранение, обработку и использование данных.
## 1. Выбор контекста:
Банковское приложение: конкретно получение данных от сервиса обработки платежей.
Потенциально хотим анализировать покупательское поведение различных групп пользователей,
с желанием подбора наиболее подходящих рекомндаций группам - но этим занимаются многие команды, а мы
будем хотеть может и не продуманные предложения предлагать пользователям, но самые актуальные для них в конкретный момент - хотим опередить другие предложения. Но я буду в контексте рекомендаций рассматривать конкретно рекламу брендов - конечно, актуальна и задача реклама наших продуктов , но я слабо разбираюсь в бизнесе финансового сектора, из-за этого чуть-чуть сменю конкретно аналитики, чтобы не так активно думать над деталями бизнесовой части.
Что примерно конкретно хотим считать в итоговой статистике - давайте разобьём товары на 2 категории: то, что клиент покупает с некой переодичностью уже приличный срок(1) и разовые покупки(2).
1) Так как мы снова ориентируемся на самые свежие рекомендации может показаться что эта категория не для нас - но, что если клиент раз в месяц (например) покупал мыло одной фирмы - а в последний месяц купил другой - у этого может быть много причин, но одна из возможных - какой-то фактор могу побудить сменить марку - и в данный момент открылся некоторый период неопределённости после котрого возможно мы снова получим поток покупок - и пока человек может вот только только оплатил предыдущую покупку, что если он увидит ещё интересные аналоги - всё познаётся в сравнии и именно в блишайшие моменты наибольший шанс заказать наш аналог тоже.
2) Разовые покупки явно основная наша цель - если это небольшая вещь, то аналогично с целью сравнения купить альтернативу вероятно будет присутсвовать желание. И тут есть ещё один сюжет - составляющие/дополняющие продукты - например клиент купил новую мышку - наверное в данный момент повышенный шанс того, что он будет не против обновить и коврик/прочую переферию, или же купил принтер - наверное он в скором времени закажет бумагу и краски и тд.

А теперь обощим нашу задачу на группы - давайте предположим, что у людей из схожих сфер схожего возвраста схожие потребности - давайте не впоследствии покупки предлагать конкретному, а так же предлагать людям из той же группы.
Итого:
хотим получать детали платежа, а также конкретный идентификатор продукта, id клиента из тразакции. Из прочих источников получать информацию о продукте, о возврасте клиента, о сфере деятельности.
хотим считать статистику по типам продуктов, которые покупает когорта.

• Какой предполагаемый объем? (Сколько ГБ/ТБ в день? Сколько
событий/записей в секунду?) - нам хочется работать в прайм тайм - gpt говорит, что у , например, сбербанка ~2000-3000 транзакций в секунду
в разгар дня - в то время как ближе к вечеру и утру частота падает в десятки раз - в целях экономии было бы хорошо учесть желаемую простоту масштабирования и обратно. Но размер конкретной записи не такой большой - из тяжёлых данных метки/теги товаров - предположу для простоты преувеличенно - 1000 байт на запись те 1000*500(средняя чистота тр за день)*60*60*24=40.23гб сырых данных в день - будем хотеть эффективное сжатие и недорогую цену хранения в выбранных технологиях.
• Какой формат данных на выходе из источника? - я далее планирую использовать debezium (те глобально изначальный источник - пусть например PostgreSQL, непосредственным источник, если говорить строго, является её WAL файл) - из-за этого легко получить json c описанием события, не думаю, что критично упираться и конвертировать в что-то бинарное - но было бы чуть лучше и с минимальными усилиями использовать avro, те стоит обратить внимание по возможности - тк много численных данных.
• Какова критичность источника? В целом не критично - да мы недополучим денег от конкретных клиентов, возможно ошибёмся, но не сделаем важных неверных выводов/шагов.
• Насколько важна свежесть? Крайне важна - напрямую следует из тз. (хотя тз нет, ну образно предположим есть)
• Как обрабатывать разные форматы источников? - я далее планирую использовать debezium что позволит не сильно зависеть от самой бд, а так же Apache flink в котором, если что, сможем попреобразовывать данные из различных источников к одному виду.
## 2. Требования к архитектуре:
Ваша схема или описание должны включать следующие слои и компоненты:
### •Источники данных: PostgreSQL база в которую добавляется информация о каждом платеже с полезной мета информацией
### •Слой приема и передачи данных: kafka+s3
Почему так? Хочу использовать debezium чтобы легко ловить обновления в бд сервиса - он будет кидать в кафку. Из кафки буду перекладывать в s3 - буду использовать Apache Flink так как он позиционирует себя как эффективный фреймворк в контексте стриминговой обработки.
### •Слой обработки и преобразования данных: s3 + Apache Hudi
Хотим складывать куда-то все данные до добавления в аналитическую сущность, но не строить тут умной логики - какое-нибудь объектное хранилище, но вот чтобы обращения к нему были не совсем медленными хотим чтобы оно метаданные само поддерживало - воспользуемся чем-то Iceberg-подобным - я думаю, что Apache Hudi хороший выбор снова в контексте стриминговой обработки.

• Где будет храниться информация о схемах таблиц и их местоположении? - за это отвечает Apache Hudi.
• Как отследить происхождение данных? - это поддерживает payload debezium. Если в этом будет смысл, то его можно будет подмешать в Flink в данные.

### •Слой обслуживания данных и аналитики: StarRocks
Если нам крайне важна актуальность - то выбор по-идее должен бы падать в сторону MPP баз вида GP/Vertica.
Но что GP, что Vertica - имеют свои ограниечения в формате использования и поддержки в большой компании.
Так что я пойду смотреть в сторону набирающих альтернатив - StarRocks - быстрый движок поддерживающий SQL запросы,
который в том числе может работать поверх Apache Hudi - по нашим критериям подходит.

• Какие основные трансформации нужны? (Очистка, обогащение, дедубликация, агрегация?) - да в целом все нужны - дедупликация и очистка в контексте фильтрации ошибочных/повторных транзаций. Обогащение - в контексте добавления метаданных клиентов, продуктов. Агрегация - в контексте подсчёта для когорт а не отдельных клиентов

### •BI слой
(я предположу, что он нужен и вероятно я немного не идеально корректно понял мысль автора дз относительно предыдущего слоя - тк слой обслуживания (core) и вот его возможно назвать аналитическим, но хочется получить красивые витрины всё-таки в чём-то предначначенным для этого)
В целом, мне кажется, что BI слой - исключительная вкусовщина, и возможно пусть есть небольшие отличия в удобствах и работе, но глобально критическая разница только в нативной поддержке коннекторов до сущности, что будет эти запросы обрабатывать (и то не принципиально, но жизнь чуть легче) - так что я продолжу линейку Apache и выберу Apache superset - у него как раз есть поддержка starRocks и создавать графики приемлимо по сложности.
• Как потребители найдут нужные данные? - готовые таблицы будут в Apache superset, предположения и аналитику можно будет тестировать отправляя запросы в starRocks - возможно для последнего стоит будет сделать сущность посредника, но в данный момент не буду это рассматривать. Сырые данные можно будет получить запросами к Hudi.

### •Оркестрация — опционально, для повышенной сложности: Apache Airflow
• Что будет запускать и координировать весь пайплайн? - Apache Airflow
• Как управлять расписанием и зависимостями задач? - в Apache Airflow можно создавать как начальные cron job-ы , так и строить зависимые задачи - это всё кодово задаётся.
• Как контролировать версию определенной задачи? - я не до конца осознаю как лично я смиог бы это использовать, но в теории , буду рассматривать нужду отката и для этого использовать feature флаги - при запуске в Airflow возможно задать конфиг в который и можно добавить нужные feature флаги - а в исполняемых задчах хранить обе реализации, выбирая нужную при запуске.

• Как аутентифицируются и авторизуются пользователи/системы? - в starrocks,s3,superset есть как дефолтные роли, так можно добавлять свои c контролем возможных операций над фиксированными данными. В данный момент я бы использовал 3 роли: администратор, дата инженер, аналитик. Администратор - пусть полный доступ.Инженер - без доступа к приватным данным, но с доступом до создания новых суущностей, аналитическим запросам. Аналитик - без доступа к приватным данным, с доступом запросов к hudi,starocks, с доступом создания сущностей только в superset
• Как разграничить доступ к разным слоям данных? - ролями

• Когда и как запускаются проверки?
Формат, целостность, базовые ограничения типов: apache flink (+ проверка схемы, если всё-таки добавится avro)
DDL проверки, возможно аггрегационные: starRocks
• Что делать, если проверка не пройдена?
В целом потеря данных не критична - давайте отбрасывать и писать warn сообщения в мониторинговые сервисы - но в целом можно и просто в логи, чтобы в данный момент не сильно об этом задумываться.

# 3. Формат представления: parquet
Транзакции - это ниша где много чисел и повторяющихся данных - вот крайне было бы неплохо и второе учесть добавив какое-нибудь сжатие,
и первое учесть, добавив бинарное представление и на представление схемы много не тратить - тк транзакций в целом очень много, в таком случае схема будет ощутима - parquet позволит учесть наши пожелания.
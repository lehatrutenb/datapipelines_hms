{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84fb5bab-2f03-43d8-be97-0cfe9bb17fa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (25.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: duckdb in /usr/local/lib/python3.8/dist-packages (1.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.24.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2025.2)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 tzdata-2025.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: fastavro in /usr/local/lib/python3.8/dist-packages (1.9.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install  duckdb\n",
    "%pip install   numpy\n",
    "%pip install  pandas\n",
    "%pip install fastavro\n",
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c977f5f2-d553-40e0-92b5-08e28aae66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect(\"benchmark.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c83031d5-1fc8-4971-87aa-030a7f7e248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "def gen_random_int_dataframe(scale_factor):\n",
    "    target_bytes = int(scale_factor*1024**3)  # 1GB\n",
    "    bytes_per_row = 10 * 8  # 10 int64 columns\n",
    "    n_rows = target_bytes // bytes_per_row\n",
    "\n",
    "    print(f\"  - Estimated raw size: {n_rows * bytes_per_row / 1024**3:.2f} GB\")\n",
    "\n",
    "    schema = {\n",
    "        'doc': '',\n",
    "        'name': 'table',\n",
    "        'namespace': '',\n",
    "        'type': 'record',\n",
    "        'fields': [{'name': f'column_{i}', 'type': 'int'} for i in range(10)]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame with random integers\n",
    "    df = pd.DataFrame({\n",
    "        f'column_{i}': np.random.randint(\n",
    "            low=np.iinfo(np.int32).min,\n",
    "            high=np.iinfo(np.int32).max,\n",
    "            size=n_rows,\n",
    "            dtype=np.int64\n",
    "        ) for i in range(10)\n",
    "    })\n",
    "    return df, schema\n",
    "\n",
    "\n",
    "def gen_random_string(length):\n",
    "    return ''.join([chr(random.randint(30, 256)) for i in range(length)])\n",
    "\n",
    "\n",
    "def gen_random_string_dataframe(scale_factor, string_size=16):\n",
    "    target_bytes = int(scale_factor*1024**3)  # 1GB\n",
    "    bytes_per_row = 10 * string_size  # 10 string columns of string_size bytes\n",
    "    n_rows = target_bytes // bytes_per_row\n",
    "\n",
    "    print(f\"  - Estimated raw size: {n_rows * bytes_per_row / 1024**3:.2f} GB\")\n",
    "    \n",
    "    schema = {\n",
    "        'doc': '',\n",
    "        'name': 'table',\n",
    "        'namespace': '',\n",
    "        'type': 'record',\n",
    "        'fields': [{'name': f'column_{i}', 'type': 'string'} for i in range(10)]\n",
    "    }\n",
    "\n",
    "    # Create DataFrame with random integers\n",
    "    df = pd.DataFrame({\n",
    "        f'column_{i}': [gen_random_string(string_size) for _ in range(n_rows)] for i in range(10)\n",
    "    })\n",
    "    return df, schema\n",
    "\n",
    "\n",
    "def gen_dicted_string_dataframe(scale_factor, dict_size=16,string_size=16):\n",
    "    target_bytes = int(scale_factor*1024**3)  # 1GB\n",
    "    bytes_per_row = 10 * string_size  # 10 string columns of string_size bytes\n",
    "    n_rows = target_bytes // bytes_per_row\n",
    "    \n",
    "    dicted_strings = [gen_random_string(string_size) for _ in range(dict_size)]\n",
    "\n",
    "    print(f\"  - Estimated raw size: {n_rows * bytes_per_row / 1024**3:.2f} GB\")\n",
    "    \n",
    "    schema = {\n",
    "        'doc': '',\n",
    "        'name': 'table',\n",
    "        'namespace': '',\n",
    "        'type': 'record',\n",
    "        'fields': [{'name': f'column_{i}', 'type': 'string'} for i in range(10)]\n",
    "    }\n",
    "\n",
    "    # Create DataFrame with random integers\n",
    "    df = pd.DataFrame({\n",
    "        f'column_{i}': [dicted_strings[random.randint(0, dict_size - 1) % dict_size] for _ in range(n_rows)] for i in range(10)\n",
    "    })\n",
    "    return df, schema\n",
    "\n",
    "\n",
    "def gen_random_date(start_date=date(2010, 1, 1), end_date=date(2025, 1, 1)):\n",
    "    start_ordinal = start_date.toordinal()\n",
    "    end_ordinal = end_date.toordinal()\n",
    "    \n",
    "    random_ordinal = random.randint(start_ordinal, end_ordinal)\n",
    "    \n",
    "    return date.fromordinal(random_ordinal)\n",
    "\n",
    "\n",
    "def gen_random_dates_dataframe(scale_factor):\n",
    "    target_bytes = int(scale_factor*1024**2)  # 1GB\n",
    "    bytes_per_row = 10 * 30  # 10 string columns of string_size bytes\n",
    "    n_rows = target_bytes // bytes_per_row\n",
    "    \n",
    "    schema = {\n",
    "        'doc': '',\n",
    "        'name': 'table',\n",
    "        'namespace': '',\n",
    "        'type': 'record',\n",
    "        'fields': [{'name': f'column_{i}', 'type': 'date'} for i in range(10)]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        f'column_{i}': [gen_random_date() for _ in range(n_rows)] for i in range(10)\n",
    "    })\n",
    "    return df, schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "94ce4db8-f6b7-4223-9b4a-d0381d998ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Estimated raw size: 0.01 GB\n",
      "  - Estimated raw size: 0.01 GB\n",
      "  - Estimated raw size: 0.01 GB\n",
      "  - Estimated raw size: 0.02 GB\n",
      "  - Estimated raw size: 0.02 GB\n",
      "  - Estimated raw size: 0.02 GB\n"
     ]
    }
   ],
   "source": [
    "tables_scaled = {}\n",
    "for scale in [0.01, 0.1, 1, 5]: # 2, 3\n",
    "    tables_scaled[scale] = {\"ints\": gen_random_int_dataframe(scale), \"random_strings\": gen_random_string_dataframe(scale), \"dicted_strings\": gen_dicted_string_dataframe(scale), \"dates\": gen_random_dates_dataframe(scale)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f57e7-2665-422b-8db8-26432b2ee3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_name(format_name, table_type, compression=\"default\"):\n",
    "    return f\"{table_type}_compression_{compression}_{format_name}\"\n",
    "def get_file_name(format_name, table_type, compression=\"default\"):\n",
    "    return  f\"{table_type}_compression_{compression}.{format_name}\"\n",
    "def get_s3_path(file_name):\n",
    "    return \"s3a://benchmark/\"+file_name\n",
    "\n",
    "AVRO = \"avro\"\n",
    "ORC = \"orc\"\n",
    "PARQUET = \"parquet\"\n",
    "FILE_SCHEMAS = [AVRO, ORC,PARQUET]\n",
    "compressions = {ORC: [\"zstd\", \"snappy\", \"lz4\", \"none\", \"zlib\"], AVRO: [\"snappy\", \"deflate\", \"bzip2\", \"zstandard\", \"xz\"], PARQUET: [\"snappy\", \"gzip\", \"zstd\", \"lz4\", \"none\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5431f32f-1221-41e9-8199-bcd889f3f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "TABLE_WRITE_TIME=\"TABLE_WRITE_TIME_SEC\"\n",
    "TABLE_FULL_SCAN_TIME=\"TABLE_FULL_SCAN_TIME_SEC\"\n",
    "prev_time_start = {}\n",
    "\n",
    "def measureTimeStart(metric):\n",
    "    prev_time_start[metric] = time.perf_counter()\n",
    "\n",
    "def measureTimeEnd(metric, data):\n",
    "    data[metric] = time.perf_counter() - prev_time_start[metric]\n",
    "\n",
    "time_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec330706-a363-4b46-92e2-ebd5536a0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "FILE_SIZE_MB=\"FILE_SIZE_MB\"\n",
    "DF_DICT_TYPE=\"records\"\n",
    "size_metrics = {}\n",
    "\n",
    "def measureFileSize(file, data):\n",
    "    data[FILE_SIZE_MB] = os.path.getsize(file)/1e6 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f499752b-934d-4096-aff1-7b8a27c2edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_schema in FILE_SCHEMAS:\n",
    "    time_metrics[cur_schema] = {}\n",
    "    size_metrics[cur_schema] = {}\n",
    "    for scale, tables in tables_scaled.items():\n",
    "        for table_name, table_schema in tables.items():\n",
    "            for compression in compressions[cur_schema] + [\"default\"]:\n",
    "                time_metrics[cur_schema][get_table_name(cur_schema, table_name,compression)] = {}\n",
    "                size_metrics[cur_schema][get_table_name(cur_schema, table_name,compression)] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c4d7948a-13ad-4d44-8094-aaa1bb34f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf8fa65f-2a24-4886-ab99-4224c1fcd2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import col, count, avg, sum, min, max, desc\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO Write\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.spark:spark-avro_2.12:3.5.3\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio-server:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"60000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.socket.timeout\", \"60000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3.connection.timeout\", \"60000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3.socket.timeout\", \"60000\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"1g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7d57298e-3323-4d42-bdb8-609fb9bec65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TABLE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "461023b2-b800-4dc6-a796-d243248c785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 13:50:15 WARN TaskSetManager: Stage 235 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:50:17 WARN TaskSetManager: Stage 236 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:50:19 WARN TaskSetManager: Stage 237 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:50:21 WARN TaskSetManager: Stage 238 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:50:23 WARN TaskSetManager: Stage 239 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:50:44 WARN TaskSetManager: Stage 245 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:50:51 WARN TaskSetManager: Stage 246 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:50:58 WARN TaskSetManager: Stage 247 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:51:06 WARN TaskSetManager: Stage 248 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:51:13 WARN TaskSetManager: Stage 249 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:51:19 WARN TaskSetManager: Stage 250 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:51:23 WARN TaskSetManager: Stage 251 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:51:27 WARN TaskSetManager: Stage 252 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:51:32 WARN TaskSetManager: Stage 253 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:51:36 WARN TaskSetManager: Stage 254 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# %%timeit -r 1 -n 2\n",
    "from fastavro import writer, parse_schema\n",
    "\n",
    "cur_schema=AVRO\n",
    "for scale, tables in tables_scaled.items():\n",
    "    for base_table_name, table_schema in tables.items():\n",
    "        for compression in compressions[cur_schema]:\n",
    "            if base_table_name == \"dates\" and cur_schema == AVRO:\n",
    "                continue\n",
    "            table, schema = table_schema\n",
    "    \n",
    "            records = table.to_dict(DF_DICT_TYPE)\n",
    "            measureTimeStart(TABLE_WRITE_TIME)\n",
    "            parsed_schema = parse_schema(schema)\n",
    "    \n",
    "            file_name=get_file_name(cur_schema, base_table_name,compression)\n",
    "            table_name = get_table_name(cur_schema, base_table_name,compression)\n",
    "            #with open(file_name, 'wb') as file:\n",
    "            #    writer(file, parsed_schema, records)\n",
    "            spark_df = spark.createDataFrame(table)\n",
    "            spark_df.write.format(\"avro\").mode(\"overwrite\").option(\"compression\", compression).save(file_name) # get_s3_path(file_name))\n",
    "            \n",
    "            measureTimeEnd(TABLE_WRITE_TIME, time_metrics[cur_schema][table_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cf6698cc-b76a-4884-a74a-b388028c883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 13:52:24 WARN TaskSetManager: Stage 265 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:52:26 WARN TaskSetManager: Stage 266 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:52:28 WARN TaskSetManager: Stage 267 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:52:30 WARN TaskSetManager: Stage 268 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:52:33 WARN TaskSetManager: Stage 269 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:52:49 WARN TaskSetManager: Stage 275 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:52:57 WARN TaskSetManager: Stage 276 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:53:04 WARN TaskSetManager: Stage 277 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:53:11 WARN TaskSetManager: Stage 278 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:53:19 WARN TaskSetManager: Stage 279 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:53:23 WARN TaskSetManager: Stage 280 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:53:27 WARN TaskSetManager: Stage 281 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:53:31 WARN TaskSetManager: Stage 282 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:53:35 WARN TaskSetManager: Stage 283 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:53:40 WARN TaskSetManager: Stage 284 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# %%timeit -r 1 -n 2\n",
    "from fastavro import writer, parse_schema\n",
    "\n",
    "cur_schema=PARQUET\n",
    "for scale, tables in tables_scaled.items():\n",
    "    for base_table_name, table_schema in tables.items():\n",
    "        for compression in compressions[cur_schema]:\n",
    "            if base_table_name == \"dates\" and (cur_schema == AVRO or cur_schema == PARQUET):\n",
    "                continue\n",
    "            table, schema = table_schema\n",
    "    \n",
    "            records = table.to_dict(DF_DICT_TYPE)\n",
    "            measureTimeStart(TABLE_WRITE_TIME)\n",
    "            parsed_schema = parse_schema(schema)\n",
    "    \n",
    "            file_name=get_file_name(cur_schema, base_table_name,compression)\n",
    "            table_name = get_table_name(cur_schema, base_table_name,compression)\n",
    "            #with open(file_name, 'wb') as file:\n",
    "            #    writer(file, parsed_schema, records)\n",
    "            spark_df = spark.createDataFrame(table)\n",
    "            spark_df.write.format(\"parquet\").mode(\"overwrite\").option(\"compression\", compression).save(file_name) # get_s3_path(file_name))\n",
    "            \n",
    "            measureTimeEnd(TABLE_WRITE_TIME, time_metrics[cur_schema][table_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3948bc4a-6f31-48f5-8e42-ca08a5cf526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2453307a-e6e5-4e81-9145-414f496fe588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 13:54:19 WARN TaskSetManager: Stage 295 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:54:21 WARN TaskSetManager: Stage 296 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:54:23 WARN TaskSetManager: Stage 297 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:54:25 WARN TaskSetManager: Stage 298 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:54:26 WARN TaskSetManager: Stage 299 contains a task of very large size (1713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:54:44 WARN TaskSetManager: Stage 310 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:54:50 WARN TaskSetManager: Stage 311 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:54:57 WARN TaskSetManager: Stage 312 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:55:05 WARN TaskSetManager: Stage 313 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:55:12 WARN TaskSetManager: Stage 314 contains a task of very large size (1229 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:55:15 WARN TaskSetManager: Stage 315 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:55:19 WARN TaskSetManager: Stage 316 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:55:23 WARN TaskSetManager: Stage 317 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:55:27 WARN TaskSetManager: Stage 318 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/02/13 13:55:30 WARN TaskSetManager: Stage 319 contains a task of very large size (3416 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "cur_schema=ORC\n",
    "for scale, tables in tables_scaled.items():\n",
    "    for base_table_name, table_schema in tables.items():\n",
    "        for compression in compressions[cur_schema]:\n",
    "            if base_table_name == \"dates\" and (cur_schema == AVRO or cur_schema == PARQUET):\n",
    "                continue\n",
    "            table, schema = table_schema\n",
    "            file_name=get_file_name(cur_schema, base_table_name,compression)\n",
    "            table_name = get_table_name(cur_schema, base_table_name,compression)\n",
    "    \n",
    "            measureTimeStart(TABLE_WRITE_TIME)\n",
    "            df_spark = spark.createDataFrame(table)\n",
    "            df_spark.write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"compression\", compression) \\\n",
    "                .orc(file_name)\n",
    "                #.orc(f\"s3a://benchmark/{file_name}\")\n",
    "\n",
    "            measureTimeEnd(TABLE_WRITE_TIME, time_metrics[cur_schema][table_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0acb7076-fa76-4e44-8878-15e55f5e4843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END TABLE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e40c3f7c-f20d-4b1d-bdc4-45f966c26b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avro': {'ints_compression_snappy_avro': {'TABLE_WRITE_TIME_SEC': 6.997193843000787},\n",
       "  'ints_compression_deflate_avro': {'TABLE_WRITE_TIME_SEC': 6.919648304000475},\n",
       "  'ints_compression_bzip2_avro': {'TABLE_WRITE_TIME_SEC': 7.061260253},\n",
       "  'ints_compression_zstandard_avro': {'TABLE_WRITE_TIME_SEC': 6.961667114999727},\n",
       "  'ints_compression_xz_avro': {'TABLE_WRITE_TIME_SEC': 8.709090909000224},\n",
       "  'ints_compression_default_avro': {},\n",
       "  'random_strings_compression_snappy_avro': {'TABLE_WRITE_TIME_SEC': 3.6337204010005735},\n",
       "  'random_strings_compression_deflate_avro': {'TABLE_WRITE_TIME_SEC': 3.7653017679995173},\n",
       "  'random_strings_compression_bzip2_avro': {'TABLE_WRITE_TIME_SEC': 4.207488721999653},\n",
       "  'random_strings_compression_zstandard_avro': {'TABLE_WRITE_TIME_SEC': 3.660791278999568},\n",
       "  'random_strings_compression_xz_avro': {'TABLE_WRITE_TIME_SEC': 7.488386822000393},\n",
       "  'random_strings_compression_default_avro': {},\n",
       "  'dicted_strings_compression_snappy_avro': {'TABLE_WRITE_TIME_SEC': 3.2711507379999603},\n",
       "  'dicted_strings_compression_deflate_avro': {'TABLE_WRITE_TIME_SEC': 3.2648819930000172},\n",
       "  'dicted_strings_compression_bzip2_avro': {'TABLE_WRITE_TIME_SEC': 3.9101618719996623},\n",
       "  'dicted_strings_compression_zstandard_avro': {'TABLE_WRITE_TIME_SEC': 3.305953547000172},\n",
       "  'dicted_strings_compression_xz_avro': {'TABLE_WRITE_TIME_SEC': 6.544661736000307},\n",
       "  'dicted_strings_compression_default_avro': {},\n",
       "  'dates_compression_snappy_avro': {},\n",
       "  'dates_compression_deflate_avro': {},\n",
       "  'dates_compression_bzip2_avro': {},\n",
       "  'dates_compression_zstandard_avro': {},\n",
       "  'dates_compression_xz_avro': {},\n",
       "  'dates_compression_default_avro': {}},\n",
       " 'orc': {'ints_compression_zstd_orc': {'TABLE_WRITE_TIME_SEC': 7.085152777000076},\n",
       "  'ints_compression_snappy_orc': {'TABLE_WRITE_TIME_SEC': 6.8937892699996155},\n",
       "  'ints_compression_lz4_orc': {'TABLE_WRITE_TIME_SEC': 6.945467094000378},\n",
       "  'ints_compression_none_orc': {'TABLE_WRITE_TIME_SEC': 7.2799213299995245},\n",
       "  'ints_compression_zlib_orc': {'TABLE_WRITE_TIME_SEC': 7.216919155999676},\n",
       "  'ints_compression_default_orc': {},\n",
       "  'random_strings_compression_zstd_orc': {'TABLE_WRITE_TIME_SEC': 3.7417913050003335},\n",
       "  'random_strings_compression_snappy_orc': {'TABLE_WRITE_TIME_SEC': 3.7018196489998445},\n",
       "  'random_strings_compression_lz4_orc': {'TABLE_WRITE_TIME_SEC': 3.6744718099998863},\n",
       "  'random_strings_compression_none_orc': {'TABLE_WRITE_TIME_SEC': 3.7597666250003385},\n",
       "  'random_strings_compression_zlib_orc': {'TABLE_WRITE_TIME_SEC': 3.818673807999403},\n",
       "  'random_strings_compression_default_orc': {},\n",
       "  'dicted_strings_compression_zstd_orc': {'TABLE_WRITE_TIME_SEC': 3.4068729749997146},\n",
       "  'dicted_strings_compression_snappy_orc': {'TABLE_WRITE_TIME_SEC': 3.4116145110001526},\n",
       "  'dicted_strings_compression_lz4_orc': {'TABLE_WRITE_TIME_SEC': 3.35169647299972},\n",
       "  'dicted_strings_compression_none_orc': {'TABLE_WRITE_TIME_SEC': 3.564975262999724},\n",
       "  'dicted_strings_compression_zlib_orc': {'TABLE_WRITE_TIME_SEC': 3.490020447000461},\n",
       "  'dicted_strings_compression_default_orc': {},\n",
       "  'dates_compression_zstd_orc': {'TABLE_WRITE_TIME_SEC': 0.14105281499996636},\n",
       "  'dates_compression_snappy_orc': {'TABLE_WRITE_TIME_SEC': 0.19472359299925301},\n",
       "  'dates_compression_lz4_orc': {'TABLE_WRITE_TIME_SEC': 0.18569833199944696},\n",
       "  'dates_compression_none_orc': {'TABLE_WRITE_TIME_SEC': 0.1880348820004656},\n",
       "  'dates_compression_zlib_orc': {'TABLE_WRITE_TIME_SEC': 0.19742900900018867},\n",
       "  'dates_compression_default_orc': {}},\n",
       " 'parquet': {'ints_compression_snappy_parquet': {'TABLE_WRITE_TIME_SEC': 6.8949354490005135},\n",
       "  'ints_compression_gzip_parquet': {'TABLE_WRITE_TIME_SEC': 7.1205337709998275},\n",
       "  'ints_compression_zstd_parquet': {'TABLE_WRITE_TIME_SEC': 6.964009024000006},\n",
       "  'ints_compression_lz4_parquet': {'TABLE_WRITE_TIME_SEC': 7.163169421999555},\n",
       "  'ints_compression_none_parquet': {'TABLE_WRITE_TIME_SEC': 6.920214205000775},\n",
       "  'ints_compression_default_parquet': {},\n",
       "  'random_strings_compression_snappy_parquet': {'TABLE_WRITE_TIME_SEC': 3.8003819840005235},\n",
       "  'random_strings_compression_gzip_parquet': {'TABLE_WRITE_TIME_SEC': 3.969336463000218},\n",
       "  'random_strings_compression_zstd_parquet': {'TABLE_WRITE_TIME_SEC': 3.69445388099939},\n",
       "  'random_strings_compression_lz4_parquet': {'TABLE_WRITE_TIME_SEC': 3.7117087100004937},\n",
       "  'random_strings_compression_none_parquet': {'TABLE_WRITE_TIME_SEC': 3.761824618999526},\n",
       "  'random_strings_compression_default_parquet': {},\n",
       "  'dicted_strings_compression_snappy_parquet': {'TABLE_WRITE_TIME_SEC': 3.294146663999527},\n",
       "  'dicted_strings_compression_gzip_parquet': {'TABLE_WRITE_TIME_SEC': 3.2657356970003093},\n",
       "  'dicted_strings_compression_zstd_parquet': {'TABLE_WRITE_TIME_SEC': 3.25132476699946},\n",
       "  'dicted_strings_compression_lz4_parquet': {'TABLE_WRITE_TIME_SEC': 3.299873070999638},\n",
       "  'dicted_strings_compression_none_parquet': {'TABLE_WRITE_TIME_SEC': 3.209894773999622},\n",
       "  'dicted_strings_compression_default_parquet': {},\n",
       "  'dates_compression_snappy_parquet': {},\n",
       "  'dates_compression_gzip_parquet': {},\n",
       "  'dates_compression_zstd_parquet': {},\n",
       "  'dates_compression_lz4_parquet': {},\n",
       "  'dates_compression_none_parquet': {},\n",
       "  'dates_compression_default_parquet': {}}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "12cebaa3-e9f3-4933-b79b-08224bf5a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_schema in FILE_SCHEMAS:\n",
    "    for scale, tables in tables_scaled.items():\n",
    "        for base_table_name, table_schema in tables.items():\n",
    "            for compression in compressions[cur_schema]:\n",
    "                if base_table_name == \"dates\" and (cur_schema == AVRO or cur_schema == PARQUET):\n",
    "                    continue\n",
    "                file_name=get_file_name(cur_schema, base_table_name,compression)\n",
    "                table_name = get_table_name(cur_schema, base_table_name,compression)\n",
    "                table, schema = table_schema\n",
    "    \n",
    "                measureFileSize(file_name, size_metrics[cur_schema][table_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5a1114a5-7f85-4b30-89be-950f707b410c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avro': {'ints_compression_snappy_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_deflate_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_bzip2_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_zstandard_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_xz_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_default_avro': {},\n",
       "  'random_strings_compression_snappy_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_deflate_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_bzip2_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_zstandard_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_xz_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_default_avro': {},\n",
       "  'dicted_strings_compression_snappy_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_deflate_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_bzip2_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_zstandard_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_xz_avro': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_default_avro': {},\n",
       "  'dates_compression_snappy_avro': {},\n",
       "  'dates_compression_deflate_avro': {},\n",
       "  'dates_compression_bzip2_avro': {},\n",
       "  'dates_compression_zstandard_avro': {},\n",
       "  'dates_compression_xz_avro': {},\n",
       "  'dates_compression_default_avro': {}},\n",
       " 'orc': {'ints_compression_zstd_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_snappy_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_lz4_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_none_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_zlib_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_default_orc': {},\n",
       "  'random_strings_compression_zstd_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_snappy_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_lz4_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_none_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_zlib_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_default_orc': {},\n",
       "  'dicted_strings_compression_zstd_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_snappy_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_lz4_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_none_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_zlib_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_default_orc': {},\n",
       "  'dates_compression_zstd_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dates_compression_snappy_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dates_compression_lz4_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dates_compression_none_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dates_compression_zlib_orc': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dates_compression_default_orc': {}},\n",
       " 'parquet': {'ints_compression_snappy_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_gzip_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_zstd_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_lz4_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_none_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'ints_compression_default_parquet': {},\n",
       "  'random_strings_compression_snappy_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_gzip_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_zstd_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_lz4_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_none_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'random_strings_compression_default_parquet': {},\n",
       "  'dicted_strings_compression_snappy_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_gzip_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_zstd_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_lz4_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_none_parquet': {'FILE_SIZE_MB': 0.004096},\n",
       "  'dicted_strings_compression_default_parquet': {},\n",
       "  'dates_compression_snappy_parquet': {},\n",
       "  'dates_compression_gzip_parquet': {},\n",
       "  'dates_compression_zstd_parquet': {},\n",
       "  'dates_compression_lz4_parquet': {},\n",
       "  'dates_compression_none_parquet': {},\n",
       "  'dates_compression_default_parquet': {}}}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "efea2eaf-db4b-4a43-bca4-a3113259541f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f8d2be4c190>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(cur_schema)\u001b[38;5;241m.\u001b[39mload(file_name)\n\u001b[1;32m     17\u001b[0m df\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m measureTimeEnd(TABLE_FULL_SCAN_TIME, time_metrics[cur_schema][table_name])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:204\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_records\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:2266\u001b[0m, in \u001b[0;36mDataFrame.from_records\u001b[0;34m(cls, data, index, exclude, columns, coerce_float, nrows)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     arr_columns \u001b[38;5;241m=\u001b[39m columns\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2266\u001b[0m     arrays, arr_columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coerce_float:\n\u001b[1;32m   2268\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/construction.py:840\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    837\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    838\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 840\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/construction.py:940\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m--> 940\u001b[0m     contents \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_object_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m contents, columns\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/construction.py:1067\u001b[0m, in \u001b[0;36mconvert_object_array\u001b[0;34m(content, dtype, dtype_backend, coerce_float)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             arr \u001b[38;5;241m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1067\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [convert(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m content]\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/construction.py:1067\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             arr \u001b[38;5;241m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1067\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m content]\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/construction.py:1025\u001b[0m, in \u001b[0;36mconvert_object_array.<locals>.convert\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert\u001b[39m(arr):\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1025\u001b[0m         arr \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m            \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtry_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert_to_nullable_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumpy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;66;03m# Notes on cases that get here 2023-02-15\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;66;03m# 1) we DO get here when arr is all Timestamps and dtype=None\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;66;03m# 2) disabling this doesn't break the world, so this must be\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;66;03m#    getting caught at a higher level\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m         \u001b[38;5;66;03m# 3) passing convert_datetime to maybe_convert_objects get this right\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;66;03m# 4) convert_timedelta?\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for cur_schema in FILE_SCHEMAS:\n",
    "    for scale, tables in tables_scaled.items():\n",
    "        for base_table_name, table_schema in tables.items():\n",
    "            for compression in compressions[cur_schema]:\n",
    "                if base_table_name == \"dates\" and (cur_schema == AVRO or cur_schema == PARQUET):\n",
    "                    continue\n",
    "                file_name=get_file_name(cur_schema, base_table_name,compression)\n",
    "                table_name = get_table_name(cur_schema, base_table_name,compression)\n",
    "                table, schema = table_schema\n",
    "    \n",
    "                measureTimeStart(TABLE_FULL_SCAN_TIME)\n",
    "                file = file_name + \"/*.\" + cur_schema\n",
    "                #df = duckdb.execute(f'SELECT * FROM read_avro(\"{file}\")').df()\n",
    "                df = spark.read.format(cur_schema).load(file_name)\n",
    "                df.createOrReplaceTempView(\"table\")\n",
    "                spark.sql(\"SELECT * FROM table\").toPandas()\n",
    "                measureTimeEnd(TABLE_FULL_SCAN_TIME, time_metrics[cur_schema][table_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712f73b-9e9c-4e60-a886-ddfe8a85fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f428cd4-b53a-4a40-ba02-ded7aec9769b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a5a994-701c-4436-9110-d5cb5fd6a7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749599d-d05e-4bac-a819-fa6569ff515d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7514551d-9cc0-47a2-8408-814767442376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce29e9-e77a-40e3-9759-9c5a6071a454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614253bc-5bab-4d6f-b51d-0f4fcc5a2708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2a09d-35f2-4249-be41-8aa34878a46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a19adce-f812-4f04-8da2-a48f93463825",
   "metadata": {},
   "outputs": [],
   "source": [
    " con.register(\"table\", df)\n",
    "        con.table(\"table\").write_csv(\"table.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93234361-60f0-4a83-8237-3e76977033d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to an in-memory database\n",
    "conn = duckdb.connect(':memory:')\n",
    "\n",
    "# Use the read_avro() function in a SQL query\n",
    "# You can use local file paths, or remote URLs\n",
    "avro_file_path = 'userdata1.avro' \n",
    "# Example remote file: 'https://blobs.duckdb.org/data/userdata1.avro'\n",
    "conn.from_pandas(gen_random_int_dataframe(1)).write_parquet('output.parquet')\n",
    "\n",
    "conn.write_parquet(\"out.parquet\")\n",
    "\n",
    "\n",
    "result = conn.execute(f\"SELECT * FROM read_avro('{avro_file_path}')\")\n",
    "\n",
    "\n",
    "# Fetch results into a Pandas DataFrame, Apache Arrow table, or other formats\n",
    "df = result.df() \n",
    "# or \n",
    "arrow_table = result.arrow() \n",
    "\n",
    "# Display the data\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "from fastavro import writer, parse_schema\n",
    "parsed_schema = parse_schema(schema)\n",
    "records = df.to_dict('table')\n",
    "avro_file_path = 'table.avro'\n",
    "with open(avro_file_path, 'wb') as out_file:\n",
    "    writer(out_file, parsed_schema, records)\n",
    "\n",
    "con.register(\"table\", df)\n",
    "con.table(\"table\").write_csv(\"table.csv\", header=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
